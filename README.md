<div align="center">

# ğŸ”¤ğŸ§ Transformer-Based Turkish Words' Root FinderğŸªµ

### Transformer-Based Turkish Morphological Root Extraction

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org)
[![Kaggle](https://img.shields.io/badge/Kaggle-Notebook-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://kaggle.com)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

A deep learning project that automatically extracts **roots** and **suffixes** from Turkish words using an **Encoderâ€“Decoder Transformer** architecture.

[Features](#-features) Â· [Architecture](#-model-architecture) Â· [Dataset](#-dataset) Â· [Setup](#-setup) Â· [Usage](#-usage) Â· [Results](#-results)

</div>

---

## ğŸ“Œ About

Turkish is an **agglutinative** language with a rich suffix system â€” a single word can carry multiple suffixes, dramatically changing its meaning. This project takes any Turkish word and:

- **Identifies its root** (stem)
- **Separates its suffixes**

Examples:

| Word | Root | Suffixes |
|---|---|---|
| `alÄ±yorum` | `al` (take) | `Ä±yorum` |
| `gÃ¶rmediÄŸi` | `gÃ¶r` (see) | `mediÄŸi` |
| `arkadaÅŸlarÄ±` | `arkadaÅŸ` (friend) | `larÄ±` |
| `Ã¶ÄŸretmenlerimde` | `Ã¶ÄŸret` (teach) | `menlerimde` |
| `gÃ¼lÃ¼mseyerek` | `gÃ¼l` (laugh) | `Ã¼mseyerek` |

---

## âœ¨ Features

- ğŸ”¤ **Character-level Seq2Seq** â€” Processes words character by character to generate the root
- ğŸ§  **Transformer Encoderâ€“Decoder** â€” Modern architecture based on Multi-Head Attention
- ğŸ” **Beam Search & Greedy Decoding** â€” Comparison of two decoding strategies
- ğŸ“Š **Comprehensive EDA** â€” Word length distributions, most/least frequent roots, suffix analysis
- ğŸ¯ **Attention Visualization** â€” Heatmaps showing which characters the model focuses on
- ğŸ“ˆ **Character-level Confusion Matrix** â€” Per-character error analysis
- ğŸ”§ **Post-processing** â€” Root correction and frequency-based refinement
- ğŸ“¦ **Export** â€” Root frequencies, character vocabulary, and inverse vocabulary outputs

---

## ğŸ— Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRANSFORMER MODEL                     â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     ENCODER       â”‚      â”‚      DECODER          â”‚     â”‚
â”‚  â”‚                  â”‚      â”‚                      â”‚     â”‚
â”‚  â”‚  Character Embed â”‚      â”‚  Character Embed     â”‚     â”‚
â”‚  â”‚  + Positional Encâ”‚      â”‚  + Positional Enc    â”‚     â”‚
â”‚  â”‚        â†“         â”‚      â”‚        â†“             â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚ Multi-Head â”‚  â”‚      â”‚  â”‚ Masked MHA     â”‚  â”‚     â”‚
â”‚  â”‚  â”‚ Attention  â”‚  â”‚ â”€â”€â”€â†’ â”‚  â”‚ + Cross MHA    â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚  + LayerNorm     â”‚      â”‚  + LayerNorm         â”‚     â”‚
â”‚  â”‚  + FFN           â”‚      â”‚  + FFN               â”‚     â”‚
â”‚  â”‚  Ã— 2 Layers      â”‚      â”‚  Ã— 2 Layers          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â†“                    â”‚
â”‚                            Dense (Softmax)              â”‚
â”‚                                    â†“                    â”‚
â”‚                           Predicted Root                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hyperparameters

| Parameter | Value |
|---|---|
| Embedding Dimension | 128 |
| Number of Attention Heads | 4 |
| Key Dimension | 32 |
| Encoder Layers | 2 |
| Decoder Layers | 2 |
| Optimizer | Adam (Cosine Decay) |
| Random State | 42 |

---

## ğŸ“ Dataset

The project uses a custom dataset containing **18,545** Turkish words.

**Format:** `word,root,suffixes` (CSV)

```csv
word,root,suffixes
alÄ±yorum,al,Ä±yorum
gÃ¶rmek,gÃ¶r,mek
ÅŸiddetli,ÅŸiddet,li
arkadaÅŸlarÄ±,arkadaÅŸ,larÄ±
```

### Dataset Statistics

- ğŸ“ **18,545** unique wordâ€“rootâ€“suffix triplets
- ğŸ”¤ 3 columns: `word`, `root`, `suffixes`
- ğŸ‡¹ğŸ‡· Turkish-specific characters: Ã§, ÄŸ, Ä±, Ã¶, ÅŸ, Ã¼
- ğŸ“Š Root length distributions, suffix distributions, and frequency analyses are detailed in the notebook

---

## ğŸš€ Setup

### Requirements

```bash
pip install tensorflow pandas numpy matplotlib seaborn scikit-learn
```

### Running on Kaggle (Recommended)

1. Sign in to [Kaggle](https://www.kaggle.com)
2. Create a **New Notebook**
3. Upload `turkish-words-roots-suffixes.csv` as a **Dataset**
4. Import the `transformer-based-turkish-words-root-finder.ipynb` notebook
5. Enable **GPU accelerator** (Settings â†’ Accelerator â†’ GPU)
6. Run all cells sequentially

### Running Locally

```bash
# Clone the repo
git clone [https://github.com/<your-username>/turkish-root-finder.git](https://github.com/omerfarukyuce/Transformer-Based-Turkish-Word-Root-Finder.git)
cd turkish-root-finder

# Create a virtual environment (optional)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install tensorflow pandas numpy matplotlib seaborn scikit-learn

# Launch Jupyter Notebook
jupyter notebook
```

---

## ğŸ’¡ Usage

### Notebook Workflow

The notebook executes the following steps in order:

1. **ğŸ“¥ Data Loading** â€” Read and preprocess the CSV dataset
2. **ğŸ“Š Exploratory Data Analysis (EDA)**
   - Word / root / suffix length distributions
   - Most frequent and rarest roots
   - No-suffix words and suffix length buckets
3. **ğŸ”¨ Model Building** â€” Define the Encoderâ€“Decoder Transformer network
4. **ğŸ‹ï¸ Training** â€” Train the model with checkpoint saving
5. **ğŸ“ˆ Evaluation**
   - Character-level Confusion Matrix
   - Greedy vs Beam Search comparison
   - Rare vs Frequent root performance
   - Word-level Exact Match metric
6. **ğŸ” Attention Visualization** â€” Encoderâ€“Decoder attention heatmaps
7. **ğŸ”§ Post-processing** â€” Root correction mechanism
8. **ğŸ“¦ Export** â€” Root frequencies, character vocabulary, and inverse vocabulary

### Example Prediction

After the model is trained, you can test words in the **Model Testing with Sample Words** section:

```python
# Greedy Decoding
predicted_root = predict_greedy("Ã§alÄ±ÅŸÄ±yorum")
# â†’ "Ã§alÄ±ÅŸ"

# Beam Search Decoding
predicted_root = predict_beam("Ã¶ÄŸretmenlerimde")
# â†’ "Ã¶ÄŸret"
```

---

## ğŸ“Š Results

The notebook provides the following metrics after training:

| Metric | Description |
|---|---|
| **Greedy Accuracy** | Word-level accuracy using greedy decoding |
| **Beam Search Accuracy** | Word-level accuracy using beam search |
| **Beam vs Greedy Gain** | Improvement of beam search over greedy |
| **Frequent vs Rare Gap** | Performance difference between frequent and rare roots |

> ğŸ’¡ Run the notebook to see detailed results and visualizations.

---

## ğŸ“‚ Project Structure

```
turkish-root-finder/
â”‚
â”œâ”€â”€ ğŸ““ transformer-based-turkish-words-root-finder (5).ipynb  # Main notebook
â”œâ”€â”€ ğŸ“Š turkish-words-roots-suffixes.csv                       # Dataset (18,545 words)
â”œâ”€â”€ ğŸ“‘ dataset-settings.xlsx                                  # Dataset (Excel format)
â”œâ”€â”€ ğŸ”„ convert_excel_to_csv.py                                # Excel â†’ CSV converter
â”œâ”€â”€ ğŸ” analyze_notebooks.py                                   # Notebook comparison tool
â”œâ”€â”€ âš¡ optimize_notebook.py                                   # Notebook optimization script
â””â”€â”€ ğŸ“– README.md                                              # This file
```

---

## ğŸ›  Utility Scripts

| Script | Description |
|---|---|
| `convert_excel_to_csv.py` | Converts the Excel file to CSV. Automatically fixes Turkish character issues (DOÄRU/YANLIÅ â†’ TRUE/FALSE conversion by Excel). |
| `analyze_notebooks.py` | Compares different notebook versions and lists added/removed sections. |
| `optimize_notebook.py` | Adds caching and runtime optimizations to the notebook. |

---

## ğŸ§ª Technical Details

### Data Preprocessing
- Words are tokenized at the character level
- Special tokens are used: `<` (START), `>` (END), `<PAD>` (padding)
- Duplicate records are removed

### Training Strategy
- **Cosine Decay** learning rate schedule
- **Checkpoint** saving for best weights
- Train/Test split (Random State = 42)

### Evaluation
- **Greedy Decoding**: Selects the highest-probability character at each step
- **Beam Search**: Evaluates multiple candidate paths in parallel
- **Post-processing**: Improves results using root frequency information and derivational suffix rules

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. **Fork** this repository
2. Create a new **branch** (`git checkout -b feature/new-feature`)
3. **Commit** your changes (`git commit -m 'Add new feature'`)
4. **Push** your branch (`git push origin feature/new-feature`)
5. Open a **Pull Request**

---

<div align="center">
   
## ğŸ” Code And Kaggle Link
Project: [transformer-based-turkish-words-root-finder.ipynb](https://github.com/omerfarukyuce/Transformer-Based-Turkish-Word-Root-Finder/blob/main/transformer-based-turkish-words-root-finder.ipynb)

Kaggle: [ğŸ”¤ğŸ§ Transformer-Based Turkish Words' Root FinderğŸªµ](https://www.kaggle.com/code/merfarukyce/transformer-based-turkish-words-root-finder)

## ğŸ“Š Datasets
Dataset: [Turkish words-roots-suffixes](https://www.kaggle.com/datasets/merfarukyce/turkish-words)

</div>




